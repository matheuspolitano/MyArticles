Data Alignment
Aligning data allocations to 64B boundaries can be important for several reasons. First, because the cache-line sizes on Intel Xeon processors and Knights Landing are also 64B, this can help to prevent false sharing for per-thread allocations. False sharing occurs when threads with affinity for different local caches modify different variables that are stored on the same cache line. Although the variables are not actually being shared by the threads (true sharing), modification of the cache line can require memory updates to maintain cache coherency resulting in a performance decrease. Secondly, alignment to the cache line size can also improve SIMD performance for vectorized loops since aligned accesses provide the fastest path through the memory hierarchy to the registers and core vector processing units (VPUs). In some cases, the compiler will generate separate code paths to handle the first and last iterations of a vectorized loop where not all data on a cache line is used for the calculation. For example, a loop to sum two contiguous arrays in memory requires loading the two source cache lines into registers, adding the results in the registers, and storing the register containing the result to memory. For most iterations of the vectorized loop, all data on the cache lines can be used without masking or permutation. However, for the first and last iteration, additional instructions can be required to prevent memory operations for data in the cache lines that is not relevant to the loop calculation. These additional code paths are called the peel (for special handling of data at the beginning of the loop) and remainder (for handling data at the end of the loop) code. Aligning arrays to the cache-line size can prevent execution of peel code resulting in a measureable increase in performance. Although gather and scatter operations are not necessarily sensitive to alignment when using the gather/scatter instructions, the compiler may choose to use alternate sequences when gathering multiple elements that are adjacent in memory (e.g., the x, y, and z coordinates for an atom position). These alternate sequences can be faster for aligned data.

In C++, dynamic allocations on the heap can be aligned by using _mm_malloc() and _mm_free() functions or the posix_memalign() function. For allocations on the stack, alignment is achieved using the qualifier __attribute__((aligned(n))). Because LAMMPS includes the capability to align data with posix_memalign(), enforcing alignment in our optimization package was achieved simply by defining a preprocessor symbol and adding a compile time check in the package to ensure correct alignment (see Fig. 20.3). Also, alignment qualifiers were added to a few important arrays allocated on the stack.

